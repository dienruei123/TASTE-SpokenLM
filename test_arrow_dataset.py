#!/usr/bin/env python3
"""
Test script for generated Arrow datasets with TASTE-SpokenLM dataset processing.

This script tests Arrow datasets generated by collect_mp3_txt_to_arrow.py
with the TASTE speech processing pipeline.
"""

import argparse
import logging
import sys
import importlib
from pathlib import Path

# Fix for MetadataPathFinder.invalidate_caches() error
try:
    import importlib.util
    if hasattr(importlib.util, 'invalidate_caches'):
        importlib.util.invalidate_caches = lambda: None
except:
    pass

import torch
from datasets import Dataset

# Add the project root to Python path to import taste_speech modules
sys.path.append(str(Path(__file__).parent))

try:
    from taste_speech.data.dataset import load_from_arrows, process_one_sample, REQUIRED_COLUMNS
except ImportError as e:
    print(f"Error importing taste_speech modules: {e}")
    print("Make sure you're running this from the project root directory")
    sys.exit(1)


def setup_logging(log_level: str = "INFO"):
    """Setup logging configuration"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(levelname)s - %(message)s'
    )


def test_basic_arrow_structure(arrow_path: str):
    """
    Test basic structure of the arrow dataset.
    
    Args:
        arrow_path: Path to the arrow dataset directory
    """
    logging.info(f"Testing basic arrow structure: {arrow_path}")
    
    try:
        # Load the dataset
        dataset = Dataset.load_from_disk(arrow_path)
        
        # Check basic structure
        logging.info(f"Dataset columns: {dataset.column_names}")
        logging.info(f"Dataset length: {len(dataset)}")
        
        if len(dataset) == 0:
            logging.error("Dataset is empty!")
            return False
        
        # Check first sample
        sample = dataset[0]
        logging.info(f"Sample keys: {list(sample.keys())}")
        
        # Check mp3 structure
        if 'mp3' in sample:
            mp3_data = sample['mp3']
            logging.info(f"MP3 keys: {list(mp3_data.keys())}")
            audio_array = mp3_data['array']
            logging.info(f"Audio array type: {type(audio_array)}")
            logging.info(f"Audio array length: {len(audio_array)}")
            logging.info(f"Sampling rate: {mp3_data['sampling_rate']}")
        else:
            logging.error("Missing 'mp3' field in sample")
            return False
        
        # Check json structure
        if 'json' in sample:
            json_data = sample['json']
            logging.info(f"JSON keys: {list(json_data.keys())}")
            logging.info(f"Text: {json_data['text'][:100]}...")
        else:
            logging.error("Missing 'json' field in sample")
            return False
        
        # Check s3_token and spk_emb
        logging.info(f"S3 token length: {len(sample['s3_token'])}")
        logging.info(f"Speaker embedding length: {len(sample['spk_emb'])}")
        
        if len(sample['s3_token']) == 0:
            logging.warning("s3_token is empty - this will cause issues with TASTE processing")
        
        if len(sample['spk_emb']) == 0:
            logging.warning("spk_emb is empty - this will cause issues with TASTE processing")
        
        logging.info("Basic structure test passed")
        return True
        
    except Exception as e:
        logging.error(f"Basic structure test failed: {e}")
        return False


def test_taste_processing(arrow_path: str, whisper_processor_fpath: str, llm_tokenizer_fpath: str):
    """
    Test processing with TASTE dataset pipeline.
    
    Args:
        arrow_path: Path to the arrow dataset directory
        whisper_processor_fpath: Path to Whisper processor
        llm_tokenizer_fpath: Path to LLM tokenizer
    """
    logging.info(f"Testing TASTE processing pipeline...")
    
    try:
        # Test with load_from_arrows function
        arrow_files = [arrow_path]
        
        logging.info("Loading dataset with TASTE processing...")
        processed_dataset = load_from_arrows(
            arrow_fpath_list=arrow_files,
            whisper_processor_fpath=whisper_processor_fpath,
            llm_tokenizer_fpath=llm_tokenizer_fpath,
            streaming=False,
            num_proc=1  # Use single process for testing
        )
        
        logging.info(f"Processed dataset length: {len(processed_dataset)}")
        logging.info(f"Processed dataset columns: {processed_dataset.column_names}")
        
        # Check if all required columns are present
        missing_columns = [col for col in REQUIRED_COLUMNS if col not in processed_dataset.column_names]
        if missing_columns:
            logging.error(f"Missing required columns: {missing_columns}")
            return False
        
        # Test first sample
        sample = processed_dataset[0]
        logging.info("First processed sample structure:")
        for key, value in sample.items():
            if isinstance(value, torch.Tensor):
                logging.info(f"  {key}: tensor shape {value.shape}, dtype {value.dtype}")
            else:
                logging.info(f"  {key}: {type(value)} - {value}")
        
        logging.info("TASTE processing test passed")
        return True
        
    except Exception as e:
        logging.error(f"TASTE processing test failed: {e}")
        logging.error(f"This is likely due to empty s3_token or spk_emb fields")
        return False


def test_single_sample_processing(arrow_path: str, whisper_processor_fpath: str, llm_tokenizer_fpath: str):
    """
    Test processing a single sample manually.
    
    Args:
        arrow_path: Path to the arrow dataset directory
        whisper_processor_fpath: Path to Whisper processor
        llm_tokenizer_fpath: Path to LLM tokenizer
    """
    logging.info("Testing single sample processing...")
    
    try:
        # Load raw dataset
        dataset = Dataset.load_from_disk(arrow_path)
        sample = dataset[0]
        
        # Import required modules
        from transformers import AutoTokenizer, WhisperProcessor
        from taste_speech.modules_taste.cosyvoice.whisper_frontend import WhisperFrontend
        from functools import partial
        
        # Initialize processors
        whisper_processor = WhisperProcessor.from_pretrained(whisper_processor_fpath)
        llm_tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_fpath)
        whisper_feature_extractor = WhisperFrontend(
            whisper_model="large-v3",
            do_pad_trim=True,
            permute=True,
        )
        
        # Process single sample
        resampler_dict = {}
        processed_sample = process_one_sample(
            sample,
            resampler_dict=resampler_dict,
            whisper_processor=whisper_processor,
            llm_tokenizer=llm_tokenizer,
            whisper_feature_extractor=whisper_feature_extractor
        )
        
        logging.info("Single sample processing successful:")
        for key, value in processed_sample.items():
            if isinstance(value, torch.Tensor):
                logging.info(f"  {key}: tensor shape {value.shape}, dtype {value.dtype}")
            else:
                logging.info(f"  {key}: {type(value)}")
        
        return True
        
    except Exception as e:
        logging.error(f"Single sample processing failed: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description="Test Arrow dataset with TASTE processing")
    parser.add_argument('--arrow_path', type=str, required=True,
                       help='Path to the Arrow dataset directory')
    parser.add_argument('--whisper_processor', type=str, default='openai/whisper-large-v3',
                       help='Whisper processor model path (default: openai/whisper-large-v3)')
    parser.add_argument('--llm_tokenizer', type=str, required=True,
                       help='LLM tokenizer path (e.g., path to Llama model)')
    parser.add_argument('--test_basic', action='store_true',
                       help='Only test basic arrow structure (skip TASTE processing)')
    parser.add_argument('--log_level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='Logging level (default: INFO)')
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(args.log_level)
    
    # Check if arrow path exists
    if not Path(args.arrow_path).exists():
        logging.error(f"Arrow dataset path does not exist: {args.arrow_path}")
        sys.exit(1)
    
    success = True
    
    # Test basic structure
    logging.info("=" * 50)
    logging.info("TESTING BASIC ARROW STRUCTURE")
    logging.info("=" * 50)
    success &= test_basic_arrow_structure(args.arrow_path)
    
    if not args.test_basic:
        # Test TASTE processing
        logging.info("\n" + "=" * 50)
        logging.info("TESTING TASTE PROCESSING PIPELINE")
        logging.info("=" * 50)
        success &= test_taste_processing(args.arrow_path, args.whisper_processor, args.llm_tokenizer)
        
        # Test single sample processing
        logging.info("\n" + "=" * 50)
        logging.info("TESTING SINGLE SAMPLE PROCESSING")
        logging.info("=" * 50)
        success &= test_single_sample_processing(args.arrow_path, args.whisper_processor, args.llm_tokenizer)
    
    # Final result
    logging.info("\n" + "=" * 50)
    if success:
        logging.info("ALL TESTS PASSED ✓")
        if args.test_basic:
            logging.info("Basic structure is valid. Run without --test_basic to test TASTE processing.")
    else:
        logging.error("SOME TESTS FAILED ✗")
        sys.exit(1)


if __name__ == "__main__":
    main()