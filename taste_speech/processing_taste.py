
from typing import List, Union, Optional
import re
import os

import numpy as np
import torchaudio.compliance.kaldi as kaldi
import torchaudio
import torch
import whisper
import onnxruntime
import torch.nn.functional as F
from transformers import pipeline
from transformers import BatchFeature, ProcessorMixin
from transformers import WhisperProcessor, AutoTokenizer
from transformers.utils.hub import cached_file, cached_files
from torch.nn.utils.rnn import pad_sequence

from .modules_taste.cosyvoice.whisper_frontend import WhisperFrontend
from .configuration_taste import TasteConfig
from .modules_taste.inference_audio import VoiceGenerator


def load_wav(wav, target_sr):
    speech, sample_rate = torchaudio.load(wav)
    speech = speech.mean(dim=0, keepdim=False)
    if sample_rate != target_sr:
        assert sample_rate > target_sr, 'wav sample rate {} must be greater than {}'.format(sample_rate, target_sr)
        speech = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)(speech)
    return speech.numpy()


def pad_seq_collate_fn(batch, device=None):
    padded = {}
    for key in batch[0].keys():
        packed_list = [
            x[key][0].clone().detach() if isinstance(x[key][0], torch.Tensor) else torch.tensor(x[key][0]) 
            for x in batch
        ]
        if 'length' in key:
            padded_tensor = torch.tensor(packed_list)
        else:
            padded_tensor = pad_sequence(packed_list, batch_first=True, padding_value=0)

        padded[key] = padded_tensor.to(device) if device is not None else padded_tensor
    return padded


LLAMA_FILES = [
    f'llama_tokenizer/{fn}'
    for fn in ['tokenizer.json', 'tokenizer_config.json', 'special_tokens_map.json']
]
WHISPER_FILES = [
    f'whisper_tokenizer/{fn}'
    for fn in ['tokenizer.json', 'tokenizer_config.json', 'special_tokens_map.json', 'added_tokens.json', 'preprocessor_config.json', 'vocab.json']
]


class TasteProcessor(ProcessorMixin):
    
    attributes = ["audio_processor", "audio_tokenizer", "llm_tokenizer"]
    optional_attributes = [
        "speaker_embed_onnx_session", 
        "speech_token_onnx_session",
        "asr_pipeline", 

        "asr_on", 
        "align_on", 
        "extract_speaker_embed_on",
        "extract_speech_token_on",
    ]

    audio_processor_class = "WhisperProcessor"
    audio_tokenizer_class = "AutoTokenizer"
    llm_tokenizer_class = "AutoTokenizer"

    @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path: Union[str, os.PathLike],
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        **kwargs,
    ):
        config = TasteConfig.from_pretrained(
            pretrained_model_name_or_path, cache_dir=cache_dir, force_download=force_download,
            local_files_only=local_files_only, token=token, revision=revision, **kwargs)

        text_model_name_or_path = os.path.dirname(
            cached_files(pretrained_model_name_or_path, LLAMA_FILES)[0]
        )   # config.text_config.name_or_path
        asr_model_name_or_path = os.path.dirname(
            cached_files(pretrained_model_name_or_path, WHISPER_FILES)[0]
        ) # config.asr_config.name_or_path

        audio_processor = WhisperProcessor.from_pretrained(asr_model_name_or_path)
        audio_tokenizer = AutoTokenizer.from_pretrained(asr_model_name_or_path)
        llm_tokenizer = AutoTokenizer.from_pretrained(text_model_name_or_path)

        if kwargs.get('asr_on', True):
            asr_pipeline = cls._build_asr_pipeline(asr_model_name_or_path)
        else:
            asr_pipeline = None

        if kwargs.get('extract_speaker_embed_on', True):
            path_speaker_embed_onnx_session = cached_file(pretrained_model_name_or_path, 'cosyvoice/speaker_embed.onnx')
            speaker_embed_onnx_session = cls._build_onnx_session(path_speaker_embed_onnx_session)
        else:
            speaker_embed_onnx_session = None

        if kwargs.get('extract_speech_token_on', True):
            path_speech_token_onnx_session = cached_file(pretrained_model_name_or_path, 'cosyvoice/speech_tokenizer_v1.onnx')
            speech_token_onnx_session = cls._build_onnx_session(path_speech_token_onnx_session)
        else:
            speech_token_onnx_session = None

        processor = TasteProcessor(
            audio_processor, audio_tokenizer, llm_tokenizer,
            asr_pipeline=asr_pipeline,
            speaker_embed_onnx_session=speaker_embed_onnx_session, 
            speech_token_onnx_session=speech_token_onnx_session,
            **kwargs
        )
        return processor

    def __init__(
        self,
        audio_processor=None,
        audio_tokenizer=None,
        llm_tokenizer=None,

        speaker_embed_onnx_session=None,
        speech_token_onnx_session=None,
        asr_pipeline=None,

        asr_on=True,
        align_on=False,
        extract_speaker_embed_on=True,
        extract_speech_token_on=True,
    ):
        super().__init__(audio_processor, audio_tokenizer, llm_tokenizer, 
            asr_on=asr_on, align_on=align_on, 
            extract_speech_token_on=extract_speech_token_on,
            extract_speaker_embed_on=extract_speaker_embed_on,
            asr_pipeline=asr_pipeline,
            speaker_embed_onnx_session=speaker_embed_onnx_session,
            speech_token_onnx_session=speech_token_onnx_session,
        )

        self.whisper_feature_extractor = WhisperFrontend(
            whisper_model="large-v3",
            do_pad_trim=True,
            permute=True,
        )

    def process_text(self, words=None, text=None):
        if words is None:
            words = [' ' + w for w in re.split(r'\s', text.strip())]

        asr_token_ids = []
        asr_word_ids = []
        llm_token_ids = []
        llm_word_ids = []
        asr_tokens_per_word = []
        for i, word in enumerate(words):
            encoded_ids = self.audio_tokenizer.encode(word, add_special_tokens=False)
            for asr_token_id in encoded_ids:
                asr_token_ids.append(asr_token_id)
                asr_word_ids.append(i)
            asr_tokens_per_word.append(len(encoded_ids))

            for llm_token_id in self.llm_tokenizer.encode(word, add_special_tokens=False):
                llm_token_ids.append(llm_token_id)
                llm_word_ids.append(i)

        text_info = {
            'words': [words],
            'text': [''.join(words)]
        }

        ids_for_text = {
            'asr_token_ids': torch.tensor([asr_token_ids], dtype=torch.int64),
            'asr_token_lengths': torch.tensor([len(asr_token_ids)], dtype=torch.int32),
            'asr_word_ids': torch.tensor([asr_word_ids], dtype=torch.int32),
            'llm_token_ids': torch.tensor([llm_token_ids], dtype=torch.int64),
            'llm_token_lengths': torch.tensor([len(llm_token_ids)], dtype=torch.int32),
            'llm_word_ids': torch.tensor([llm_word_ids], dtype=torch.int32),
        }
        return text_info, ids_for_text

    def __call__(
        self,
        audio=None,
        sampling_rate=None,
        text=None,
        ref_audio_list=None,
        **kwargs,
    ) -> BatchFeature:

        data = {}

        if isinstance(audio, str):
            audio = load_wav(audio, target_sr=sampling_rate)

        if isinstance(ref_audio_list[0], str):
            ref_audio_list = [load_wav(ref_audio, target_sr=sampling_rate)
                              for ref_audio in ref_audio_list]

        assert len(audio.shape) == 1
        assert len(ref_audio_list[0].shape) == 1
        assert sampling_rate == 16000

        if self.extract_speaker_embed_on:
            speaker_embed = self._get_speaker_embed(self.speaker_embed_onnx_session, ref_audio_list)
            data.update(
                {
                    'speaker_embeds': torch.tensor([speaker_embed], dtype=torch.float32)
                }
            )
        if self.extract_speech_token_on:
            speech_token = self._get_speech_token(self.speech_token_onnx_session, audio)
            data.update(
                {
                    'speech_token_ids': torch.tensor([speech_token], dtype=torch.int64),
                    'speech_token_lengths': torch.tensor([len(speech_token)], dtype=torch.int32)
                }
            )
        
        audio_features, audio_feature_lengths = self.whisper_feature_extractor(
            torch.tensor([audio], dtype=torch.float32), [audio.shape[0]])
        data.update({
            'audio_features': torch.tensor(audio_features, dtype=torch.float32),
            'audio_feature_lengths': torch.tensor(audio_feature_lengths, dtype=torch.int32)
        })

        words = None
        text = None
        if self.asr_on:
            result = self.asr_pipeline(
                {'raw': audio, 'sampling_rate': sampling_rate},
                return_timestamps='word' if self.align_on else None,
                generate_kwargs={
                    'language': 'english',
                    'forced_decoder_ids': None,
                    'task': 'transcribe'
                },
                batch_size=1,
            )
            text = result['text']
        elif text:
            text = re.sub(r'\s', ' ', text)
        else:
            raise ValueError("`text` is needed")

        if self.align_on:
            words = [chunk['text'] for chunk in result['chunks']]

        text_info, ids_for_text = self.process_text(words=words, text=text)
        data.update(ids_for_text)
        if kwargs.pop('output_text_info', False):
            data.update(text_info)

        if self.align_on:
            audio_length = audio.shape[0] / sampling_rate
            alignment = []
            for word_id, x in enumerate(result['chunks']):
                span_range = [t / audio_length for t in x['timestamp']]
                for _ in range(asr_tokens_per_word[word_id]):
                    alignment.append(span_range)
            data.update(
                {
                    'asr_token_alignments': torch.tensor([alignment], dtype=torch.float32),
                }
            )

        return BatchFeature(data=data)

    def get_generator(self, pretrained_model_name_or_path, device='cpu'):
        generator = VoiceGenerator()
        path_generator = cached_file(pretrained_model_name_or_path, 'cosyvoice/voice_generator.pth')
        generator.load_state_dict(torch.load(path_generator, weights_only=True))
        return generator.to(device)

    @classmethod
    def _build_onnx_session(cls, onnx_path):
        option = onnxruntime.SessionOptions()
        option.inter_op_num_threads = 1
        option.intra_op_num_threads = 1
        option.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL # TODO: Should I keep this?
        providers = ["CPUExecutionProvider"]
        onnx_session = onnxruntime.InferenceSession(onnx_path, sess_options=option, providers=providers)
        return onnx_session
        
    def _get_speaker_embed(self, session, audio_list):
        embed_list = []
        for audio in audio_list:
            feat = kaldi.fbank(
                torch.tensor([audio]),
                num_mel_bins=80,
                dither=0,
                sample_frequency=16000)
            feat = feat - feat.mean(dim=0, keepdim=True)
            embed = session.run(
                None, 
                {session.get_inputs()[0].name: feat.unsqueeze(dim=0).cpu().numpy()}
            )[0].flatten().tolist()
            embed_list.append(embed)
        speaker_embed = F.normalize(torch.tensor(embed_list).mean(dim=0), dim=0).numpy()
        return speaker_embed

    def _get_speech_token(self, session, audio):
        if audio.shape[0] / 16000 > 30:
            logging.warning('do not support extract speech token for audio longer than 30s')
            speech_token = []
        else:
            feat = whisper.log_mel_spectrogram(np.array([audio]), n_mels=128)
            speech_token = session.run(None, 
                {
                    session.get_inputs()[0].name: feat.detach().cpu().numpy(),
                    session.get_inputs()[1].name: np.array([feat.shape[2]], dtype=np.int32)
                }
            )[0].flatten().tolist()
        return speech_token

    @classmethod
    def _build_asr_pipeline(cls, path):
        asr_pipeline = pipeline(
            'automatic-speech-recognition',
            model=path,
            torch_dtype=torch.float16,
            device=0,
            chunk_length_s=30,
        )
        return asr_pipeline
