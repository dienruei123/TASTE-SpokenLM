# -*- coding: utf-8 -*-
"""VoiceSense_CTC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QPi3y7yPXfS038Xgi3V3fVNyeh1UFwFC
For examinating the re-implemented functionality of sensevoice audio segmenter. 
"""

# !rm -fr FunASR/

# !git clone https://github.com/GitYCC/FunASR.git

# !pip3 install -e FunASR/

from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model_dir = "FunAudioLLM/SenseVoiceSmall"

# model = AutoModel(
#     model=model_dir,
#     device="cpu",
#     hub="hf",
# )

model = AutoModel(
    model=model_dir,
    vad_model=None,
    device="cpu",
    hub="hf",
)
# !mkdir segmented_results/

from sys import audit
from ctypes import alignment
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess
import torch

model_dir = "FunAudioLLM/SenseVoiceSmall"
audio_path = f"{model.model_path}/example/en.mp3"


res = model.generate(
    input=audio_path,
    cache={},
    language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
    use_itn=True,
    batch_size_s=60,
)[0]

tokenizer = model.kwargs['tokenizer']
tokens = tokenizer.sp.id_to_piece(res['raw'].tolist())
out_hidden = res['encoder_out']
print(tokens)

def get_alignment_from_sensevoice(tokens, window_size=9, num_prev_elements=4):
    assert window_size % 2 == 1, "window_size must be odd"

    # get center index
    left_map = []
    right_map = []
    for i in range(num_prev_elements, len(tokens)):
        token = tokens[i]
        if len(left_map) == len(right_map):
            left_map.append([token, i])
        elif token != left_map[-1][0]:
            right_map.append([tokens[i-1], i-1])
            left_map.append([token, i])
    if len(left_map) != len(right_map):
        right_map.append([tokens[-1], len(tokens)-1])
    center_map = [(token, (left + right) // 2) for (token, left), (_, right) in zip(left_map, right_map)
                  if token != '<unk>']

    # merging
    def _limit_left(index):
        return max(index, num_prev_elements)
    def _limit_right(index):
        return min(index, len(tokens))

    r = (window_size - 1) // 2
    range_map = [
        [
            center_map[0][0],
            [_limit_left(center_map[0][1] - r,), _limit_right(center_map[0][1] + r + 1)]
        ]
    ]
    i = 1
    while i < len(center_map):
        token, index = center_map[i]
        if not token.startswith('â–'):
            range_map[-1][0] += token
            range_map[-1][1][1] = _limit_right(center_map[i][1] + r + 1)
        else:
            range_map.append(
                [
                    token,
                    [_limit_left(center_map[i][1] - r,), _limit_right(center_map[i][1] + r + 1)]
                ])
        i += 1

    alignments = [(segment, (left - num_prev_elements, right - num_prev_elements))
                  for segment, (left, right) in range_map]
    return alignments

alignments = get_alignment_from_sensevoice(tokens)

# 0.06 sec per unit in SenseVoice
sec_per_unit = 0.06

import librosa
import soundfile as sf
sr = 16000
# audio, sr = librosa.load(audio_path, sr=sr)

for i, (segment, (start, end)) in enumerate(alignments):
    start = int(start * sec_per_unit * sr)
    end = int(end * sec_per_unit * sr)
    print("start:", start, "end:", end, "segment:", segment)
    # sf.write(f"segmented_results/{i}-{segment}.wav", audio[start:end], sr)

print(model.kwargs)

# !zip -r segmented_results.zip segmented_results/

# prefix_elements = 4
# tmp_start = None
# tmp_token = None

# segments = []


# def ctc_alignment(ctc_output):
#     result = []
#     current_token = None
#     start = 0

#     for i, token in enumerate(ctc_output):
#         if token != '_' and (current_token is None or token != current_token):
#             if current_token is not None:
#                 result.append([(start, i), current_token])
#             current_token = token
#             start = i
#         elif token == '_' and current_token is not None:
#             result.append([(start, i), current_token])
#             current_token = None
#             start = i

#     # handle the last character
#     if current_token is not None:
#         result.append([(start, len(ctc_output)), current_token])

#     return result


# for i, token in enumerate(tokens):
#     if i < prefix_elements:
#         continue

#     if i == prefix_elements:
#         tmp_start = i
#         if token != '<unk>':
#             tmp_token = token
#     elif token == '<unk>':
#         if tmp_start is not None:
#             segments.append((tmp_start, i))
#             tmp_start = None
#             tmp_token = None

# def ctc_alignment(seq):
#     result = []
#     start = 0
#     for i in range(len(seq)):
#         # We don't include `<unk>` in our alignment result
#         if seq[i] == '<unk>':
#             continue
#         # If we are at last element or next element is not same as current
#         if i == len(seq)-1 or seq[i] != seq[i+1]:
#             result.append([(start, i+1), seq[i]])
#             start = i+1
#     return result

tokenizer.decode(
    [24885, 25004, 24993, 25016,     68,      3045,     3862,  4871]
)

